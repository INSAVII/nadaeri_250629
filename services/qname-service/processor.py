#!/usr/bin/env python3
"""
ìƒí’ˆëª… ë° í‚¤ì›Œë“œ ìƒì„± í”„ë¡œì„¸ì„œ - ë¦¬íŒ©í„°ë§ ë²„ì „
ê°„ì†Œí™”ëœ APIì™€ ëª…í™•í•œ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import pandas as pd
import requests
import random
import time
import re
from datetime import datetime, timedelta
from dotenv import load_dotenv
import google.generativeai as genai
import logging

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ë¨¼ì € ì •ì˜)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ ì¶œë ¥
import logging.handlers

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = os.path.join(SCRIPT_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

# ë¡œê·¸ íŒŒì¼ ì„¤ì •
log_file = os.path.join(log_dir, f'qname_processor_{datetime.now().strftime("%Y%m%d")}.log')

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¼ë³„ ë¡œê·¸ íŒŒì¼)
file_handler = logging.handlers.TimedRotatingFileHandler(
    log_file, 
    when='midnight', 
    interval=1, 
    backupCount=7,
    encoding='utf-8'
)
file_handler.setLevel(logging.INFO)

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# í¬ë§·í„°
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(console_handler)

logger.info("QName Processor ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

# .env íŒŒì¼ ë¡œë“œ - ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ .env íŒŒì¼ ì°¾ê¸°
possible_env_paths = [
    ".env",  # í˜„ì¬ ë””ë ‰í† ë¦¬
    "../.env",  # ìƒìœ„ ë””ë ‰í† ë¦¬ (ë£¨íŠ¸)
    "../../.env",  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬
    os.path.join(os.path.dirname(__file__), ".env"),  # ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬
    os.path.join(os.path.dirname(__file__), "..", ".env"),  # ìƒìœ„ ë””ë ‰í† ë¦¬
    os.path.join(os.path.dirname(__file__), "..", "..", ".env"),  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬
]

# .env íŒŒì¼ ì°¾ê¸° ë° ë¡œë“œ
env_loaded = False
for env_path in possible_env_paths:
    if os.path.exists(env_path):
        load_dotenv(env_path)
        logger.info(f"í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ë¡œë“œë¨: {os.path.abspath(env_path)}")
        env_loaded = True
        break

if not env_loaded:
    logger.warning("ì–´ë–¤ .env íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    load_dotenv()  # ê¸°ë³¸ ë™ì‘

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ì „ ë¹„í™œì„±í™” (ì†ë„ ìµœì í™”)
logger.info("í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë¹„í™œì„±í™” (ì†ë„ ìµœì í™” ëª¨ë“œ)")

# ========================================
# ğŸ”‘ API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì§ì ‘ì„¤ì •ì€ ê°œë°œìš©)
# ========================================
# âš ï¸  ë³´ì•ˆ ì£¼ì˜: ì‹¤ì œ API í‚¤ëŠ” .env íŒŒì¼ì—ë§Œ ì €ì¥í•˜ì„¸ìš”!
# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ê°’ (ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ë°˜ë“œì‹œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
DIRECT_GEMINI_API_KEY = ""  # â† .env íŒŒì¼ì— GEMINI_API_KEY ì„¤ì •
DIRECT_NAVER_CLIENT_ID = ""  # â† .env íŒŒì¼ì— NAVER_CLIENT_ID ì„¤ì •  
DIRECT_NAVER_CLIENT_SECRET = ""  # â† .env íŒŒì¼ì— NAVER_CLIENT_SECRET ì„¤ì •

# ========================================
# API í‚¤ ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ > ì§ì ‘ì„¤ì • > ì—†ìŒ
# ========================================
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì½”ë“œ ì„¤ì • fallback)
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET')

# í™˜ê²½ë³€ìˆ˜ê°€ ì—†ì„ ë•Œë§Œ ì§ì ‘ì„¤ì • ì‚¬ìš© (ê°œë°œìš©)
if not GEMINI_API_KEY:
    GEMINI_API_KEY = DIRECT_GEMINI_API_KEY

if not NAVER_CLIENT_ID:
    NAVER_CLIENT_ID = DIRECT_NAVER_CLIENT_ID

if not NAVER_CLIENT_SECRET:
    NAVER_CLIENT_SECRET = DIRECT_NAVER_CLIENT_SECRET

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ
logger.info("í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")
logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {SCRIPT_DIR}")

class QNameProcessor:
    """QName ì²˜ë¦¬ê¸° - ê°„ì†Œí™”ëœ ë²„ì „"""
    
    def __init__(self):
        self.naver_url = "https://openapi.naver.com/v1/search/shop.json"
        self.category_mapper = CategoryMapper()
        
        # ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ
        if not self.category_mapper.load_category_data():
            logger.warning("ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©")
        
        # Gemini API ì„¤ì •
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            self.model = genai.GenerativeModel('models/gemini-1.5-pro-latest')
        else:
            logger.error("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.model = None
    
    def process_excel_file(self, file_path: str) -> dict:
        """ì—‘ì…€ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
        try:
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # ì—‘ì…€ íŒŒì¼ ì½ê¸°
            df = pd.read_excel(file_path)
            logger.info(f"ì´ ì²˜ë¦¬í•  í–‰ ìˆ˜: {len(df)}")
            
            if 'ë©”ì¸í‚¤ì›Œë“œ' not in df.columns:
                raise ValueError("'ë©”ì¸í‚¤ì›Œë“œ' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            processed_count = 0
            success_count = 0
            
            for index, row in df.iterrows():
                keyword = row['ë©”ì¸í‚¤ì›Œë“œ']
                start_time = time.time()
                logger.info(f"í–‰ {index+1} ì²˜ë¦¬ ì‹œì‘: {keyword}")
                
                try:
                    # 1. ë„¤ì´ë²„ APIë¡œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    step1_start = time.time()
                    category_info = self._get_naver_category(keyword)
                    step1_time = time.time() - step1_start
                    logger.info(f"  ë‹¨ê³„1(ë„¤ì´ë²„API): {step1_time:.2f}ì´ˆ")
                    
                    # 2. ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
                    step2_start = time.time()
                    category_format, core_keyword = self._extract_category_info(category_info)
                    step2_time = time.time() - step2_start
                    logger.info(f"  ë‹¨ê³„2(ì¹´í…Œê³ ë¦¬ì¶”ì¶œ): {step2_time:.2f}ì´ˆ")
                    
                    # 3. ì¹´í…Œê³ ë¦¬ ì½”ë“œ ë§¤í•‘
                    step3_start = time.time()
                    category_code, is_suspicious = self.category_mapper.find_category_code(category_format)
                    step3_time = time.time() - step3_start
                    logger.info(f"  ë‹¨ê³„3(ì¹´í…Œê³ ë¦¬ë§¤í•‘): {step3_time:.2f}ì´ˆ")
                    
                    # 4. ìƒí’ˆëª… ìƒì„± (1ë‹¨ê³„)
                    step4_start = time.time()
                    product_name = self._generate_product_name(keyword, category_format, core_keyword)
                    step4_time = time.time() - step4_start
                    logger.info(f"  ë‹¨ê³„4(ìƒí’ˆëª…ìƒì„±): {step4_time:.2f}ì´ˆ")
                    
                    # 5. ìƒí’ˆëª… ê²°ê³¼ ì €ì¥ (ì¤‘ê°„ ì €ì¥)
                    step5_start = time.time()
                    df.at[index, 'NAVERCODE'] = category_code
                    df.at[index, 'ì¹´í…Œë¶„ë¥˜í˜•ì‹'] = f"{'X' if is_suspicious else ''}{category_format}"
                    df.at[index, 'SEOìƒí’ˆëª…'] = product_name
                    df.at[index, 'ê°€ê³µê²°ê³¼'] = 'ìƒí’ˆëª…ì™„ë£Œ'
                    step5_time = time.time() - step5_start
                    logger.info(f"  ë‹¨ê³„5(ìƒí’ˆëª…ì €ì¥): {step5_time:.2f}ì´ˆ")
                    
                    # 6. ì—°ê´€ê²€ìƒ‰ì–´ ìƒì„± (2ë‹¨ê³„ - ìƒí’ˆëª… ìƒì„± í›„)
                    step6_start = time.time()
                    related_keywords = self._get_related_keywords(keyword, core_keyword, product_name)
                    step6_time = time.time() - step6_start
                    logger.info(f"  ë‹¨ê³„6(ì—°ê´€ê²€ìƒ‰ì–´ìƒì„±): {step6_time:.2f}ì´ˆ")
                    
                    naver_tags = random.sample(related_keywords, min(10, len(related_keywords)))
                    
                    # 7. ì—°ê´€ê²€ìƒ‰ì–´ ê²°ê³¼ ì €ì¥ (ìµœì¢… ì €ì¥)
                    step7_start = time.time()
                    df.at[index, 'ì—°ê´€ê²€ìƒ‰ì–´'] = ','.join(related_keywords)
                    df.at[index, 'ë„¤ì´ë²„íƒœê·¸'] = ','.join(naver_tags)
                    df.at[index, 'ê°€ê³µê²°ê³¼'] = 'ì™„ë£Œ'
                    step7_time = time.time() - step7_start
                    logger.info(f"  ë‹¨ê³„7(ì—°ê´€ê²€ìƒ‰ì–´ì €ì¥): {step7_time:.2f}ì´ˆ")
                    
                    success_count += 1
                    total_time = time.time() - start_time
                    logger.info(f"ì™„ë£Œ: {product_name} (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
                    
                except Exception as e:
                    logger.error(f"í–‰ {index+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    df.at[index, 'ê°€ê³µê²°ê³¼'] = f'ì˜¤ë¥˜: {str(e)}'
                
                processed_count += 1
                # time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ ì œê±° - ì†ë„ ìµœì í™”
            
            # ê²°ê³¼ íŒŒì¼ ì €ì¥
            output_file = os.path.join(SCRIPT_DIR, f"ê°€ê³µì™„ë£Œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            df.to_excel(output_file, index=False)
            
            logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}í–‰ ì¤‘ {success_count}í–‰ ì„±ê³µ")
            logger.info(f"ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_file}")
            
            return {
                'success': True,
                'output_file': output_file,
                'total_processed': processed_count,
                'success_count': success_count,
                'error_count': processed_count - success_count
            }
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _get_naver_category(self, keyword: str) -> dict:
        """ë„¤ì´ë²„ ì‡¼í•‘ APIë¡œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° - ëª¨ë“  í‚¤ì›Œë“œ API ì¡°íšŒ"""
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            logger.warning("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©")
            return self._create_default_category(keyword)
        
        # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë„¤ì´ë²„ APIë¡œ ì¡°íšŒ (ê¸°ë³¸ ë§¤í•‘ ì œê±°)
        logger.info(f"=== ë„¤ì´ë²„ API ì¡°íšŒ ì‹œì‘: {keyword} ===")
        
        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }
        params = {"query": keyword, "display": 1}
        
        logger.info(f"API ìš”ì²­ URL: {self.naver_url}")
        logger.info(f"API ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
        
        try:
            start_time = time.time()
            response = requests.get(self.naver_url, headers=headers, params=params, timeout=3)
            response_time = time.time() - start_time
            
            logger.info(f"API ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
            logger.info(f"API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                logger.info(f"API ì‘ë‹µ ë‚´ìš©: {result}")
                
                if 'items' in result and result['items']:
                    item = result['items'][0]
                    category_info = {
                        'category1': item.get('category1', 'N/A'),
                        'category2': item.get('category2', 'N/A'),
                        'category3': item.get('category3', 'N/A'),
                        'category4': item.get('category4', 'N/A')
                    }
                    logger.info(f"ë„¤ì´ë²„ API ì„±ê³µ: {keyword} â†’ {category_info}")
                    return result
                else:
                    logger.warning(f"ë„¤ì´ë²„ API ì‘ë‹µì— ìƒí’ˆ ì—†ìŒ: {keyword}")
                    logger.warning(f"ì‘ë‹µ ë‚´ìš©: {result}")
                    return self._create_default_category(keyword)
            else:
                logger.error(f"ë„¤ì´ë²„ API HTTP ì˜¤ë¥˜: {response.status_code} - {keyword}")
                logger.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
                return self._create_default_category(keyword)
                
        except requests.exceptions.Timeout:
            logger.error(f"ë„¤ì´ë²„ API íƒ€ì„ì•„ì›ƒ (3ì´ˆ ì´ˆê³¼): {keyword}")
            return self._create_default_category(keyword)
        except requests.exceptions.ConnectionError:
            logger.error(f"ë„¤ì´ë²„ API ì—°ê²° ì˜¤ë¥˜: {keyword}")
            return self._create_default_category(keyword)
        except requests.exceptions.RequestException as e:
            logger.error(f"ë„¤ì´ë²„ API ìš”ì²­ ì˜¤ë¥˜: {str(e)} - {keyword}")
            return self._create_default_category(keyword)
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ API ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)} - {keyword}")
            return self._create_default_category(keyword)
    
    def _create_default_category(self, keyword: str) -> dict:
        """ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ ìƒì„± - API ì‹¤íŒ¨ ì‹œì—ë§Œ ì‚¬ìš©"""
        logger.warning(f"ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš© (API ì‹¤íŒ¨): {keyword}")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì¶”ì •
        if any(word in keyword for word in ['ì–‘ë§', 'ì‹ ë°œ', 'ìš´ë™í™”', 'ìŠ¬ë¦¬í¼']):
            category = 'íŒ¨ì…˜ì˜ë¥˜>ì‹ ë°œ/ê°€ë°©>ì–‘ë§'
        elif any(word in keyword for word in ['í…€ë¸”ëŸ¬', 'ì»¤í”¼', 'ë³´ì˜¨ë³‘']):
            category = 'ì£¼ë°©ìš©í’ˆ>ì»¤í”¼ìš©í’ˆ>í…€ë¸”ëŸ¬'
        elif any(word in keyword for word in ['ì‹ê¸°', 'ê·¸ë¦‡', 'ì ‘ì‹œ']):
            category = 'ì£¼ë°©ìš©í’ˆ>ì‹ê¸°ë¥˜>ì‹ê¸°'
        elif any(word in keyword for word in ['ìº í•‘', 'ë“±ì‚°', 'ì•„ì›ƒë„ì–´']):
            category = 'ìŠ¤í¬ì¸ /ë ˆì €>ìº í•‘ìš©í’ˆ>ìº í•‘ìš©í’ˆ'
        elif any(word in keyword for word in ['ì²­ì†Œ', 'ì •ë¦¬', 'ë³´ê´€']):
            category = 'ì£¼ë°©ìš©í’ˆ>ì²­ì†Œìš©í’ˆ>ì²­ì†Œìš©í’ˆ'
        else:
            # í‚¤ì›Œë“œì—ì„œ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ ì¶”ì •
            category = 'ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ'
        
        categories = category.split('>')
        return {
            'items': [{
                'category1': categories[0] if len(categories) > 0 else 'ì£¼ë°©ìš©í’ˆ',
                'category2': categories[1] if len(categories) > 1 else 'ì£¼ë°©ìš©í’ˆ',
                'category3': categories[2] if len(categories) > 2 else 'ì£¼ë°©ìš©í’ˆ',
                'category4': categories[3] if len(categories) > 3 else 'ì£¼ë°©ìš©í’ˆ'
            }]
        }
    
    def _extract_category_info(self, category_info: dict) -> tuple:
        """ì¹´í…Œê³ ë¦¬ ì •ë³´ì—ì„œ í˜•ì‹ê³¼ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            items = category_info.get('items', [])
            if not items:
                return 'ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ', 'ì£¼ë°©ìš©í’ˆ'
            
            item = items[0]
            categories = []
            for i in range(1, 5):
                category = item.get(f'category{i}')
                if category:
                    categories.append(category)
            
            if not categories:
                return 'ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ', 'ì£¼ë°©ìš©í’ˆ'
            
            category_format = '>'.join(categories)
            core_keyword = categories[-1] if categories else 'ì£¼ë°©ìš©í’ˆ'
            
            return category_format, core_keyword
            
        except Exception as e:
            return 'ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ>ì£¼ë°©ìš©í’ˆ', 'ì£¼ë°©ìš©í’ˆ'
    
    def _generate_product_name(self, keyword: str, category_format: str, core_keyword: str) -> str:
        """Gemini APIë¡œ ìƒí’ˆëª…ë§Œ ìƒì„± - ê³ ê¸‰ 2ë‹¨ê³„ í”„ë¡¬í”„íŠ¸"""
        if not self.model:
            # ê¸°ë³¸ ëª¨ë“œ: í‚¤ì›Œë“œ ê¸°ë°˜ ìƒí’ˆëª… ìƒì„±
            return self._generate_basic_product_name(keyword, category_format, core_keyword)
        
        try:
            # 1ë‹¨ê³„: prefix(ì¡°í•©ë‹¨ì–´+core keyword) ì¶”ì²œì„ LLMì´ í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prefix_prompt = (
                f"ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìƒí’ˆëª… ì•ë¶€ë¶„ì— ë“¤ì–´ê°ˆ ê°€ì¥ ì í•©í•œ ì¡°í•©í˜• ë‹¨ì–´(ìš©ë„, ì ìš©, íŠ¹ì„±, ì¢…ë¥˜ ë“±ê³¼ core keywordì˜ ì¡°í•©)ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”. "
                f"ì‹¤ì œ ìƒí’ˆì˜ ìš©ë„ì™€ ìƒí™©ì— ë§ê³ , í˜¼ë™ì„ ìœ ë°œí•˜ì§€ ì•Šìœ¼ë©°, ì¤‘ë³µ ì—†ì´ í•œê¸€ë¡œë§Œ ì¶”ì²œí•´ ì£¼ì„¸ìš”. "
                f"ì˜ˆì‹œ: core keywordê°€ 'ì„¸íƒì†”'ì´ê³  ë©”ì¸í‚¤ì›Œë“œì— 'ë¹¨ë˜'ê°€ ìˆìœ¼ë©´ 'ë¹¨ë˜ì„¸íƒì†”', 'ìš•ì‹¤'ì´ ìˆìœ¼ë©´ 'ìš•ì‹¤ì„¸íƒì†”', 'í™ˆì„¸íŠ¸'ì™€ 'ë„ìê¸°'ê°€ ìˆìœ¼ë©´ 'ë„ìê¸°í™ˆì„¸íŠ¸' ë“±. "
                f"ì¹´í…Œë¶„ë¥˜í˜•ì‹: {category_format}\nCore keyword: {core_keyword}\në©”ì¸í‚¤ì›Œë“œ: {keyword}\n"
                f"ì¶”ì²œ prefix(ë„ì–´ì“°ê¸° ì—†ì´ í•œ ë‹¨ì–´ë¡œ):"
            )
            
            try:
                prefix_response = self.model.generate_content(prefix_prompt)
                prefix = prefix_response.text.strip().split()[0]  # ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ì‚¬ìš©
            except Exception as api_error:
                logger.error(f"Prefix ìƒì„± API ì˜¤ë¥˜: {str(api_error)}")
                prefix = self._select_best_prefix_word(category_format, core_keyword, keyword)

            # 2ë‹¨ê³„: ìƒí’ˆëª… ì „ì²´ í”„ë¡¬í”„íŠ¸ì—ì„œ prefixë¥¼ ê³ ì •
            prompt = (
                f"ìƒí’ˆëª… ì•ë¶€ë¶„ì€ '{prefix}'ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. "
                f"ì´ì–´ì„œ ìƒí’ˆì˜ ì¢…ë¥˜, ëª©ì , íŠ¹ì§•, ìš©ë„ ë“±ì„ ì¤‘ë³µ ì—†ì´, ë¸Œëœë“œ ì œì™¸, "
                f"25ì ì´ìƒ 30ì ë‚´ì™¸ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ë¶™ì¼ ë‹¨ì–´(ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„)ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”. "
                f"ë‹¨, '{prefix}'ëŠ” ë°˜ë“œì‹œ í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ì„¸ìš”. "
                f"ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”:\n"
                f"1. ì¤‘ë³µë‹¨ì–´ ì‚¬ìš©ê¸ˆì§€\n"
                f"2. íŠ¹ìˆ˜ë¬¸ì, ê¸°í˜¸, ì˜ì–´, ì½¤ë§ˆ, ê´„í˜¸ ì‚¬ìš©ê¸ˆì§€\n"
                f"3. ë‹¨ì–´ë³„ í•œ ì¹¸ ë„ì–´ì“°ê¸° ì¤€ìˆ˜\n"
                f"4. ë¸Œëœë“œë¬¸ì, ëœ»ì´ ë¶„ëª…í•˜ì§€ ì•Šì€ ë‹¨ì–´, í•œê¸€ë¡œ ì •í™•í•œ í’ˆëª©ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì‚¬ìš©ê¸ˆì§€\n"
                f"5. í•œê¸€ì˜ í’ˆëª© ë˜ëŠ” ìš©ë„, ëª©ì , íŠ¹ì§•, í˜•ìƒ, ì¢…ë¥˜ë¥¼ í‘œí˜„í•˜ëŠ” ë‹¨ì–´ ì´ì™¸ ì‚¬ìš©ê¸ˆì§€\n"
                f"6. ë™ë¬¼ìš©, ì‚¬ëŒìš©, ì–´ë¦°ì´ìš©, ì„±ì¸ìš©, ë‚¨ì„±ìš©, ì—¬ì„±ìš© ë“± ë‹¨ì¼ìƒí’ˆëª…ì— í˜¼í•© ì‚¬ìš© ê¸ˆì§€\n"
                f"7. ì¸ì¦í•„ìš” ë‹¨ì–´(ì˜ˆ: ì¹œí™˜ê²½) ì‚¬ìš© ê¸ˆì§€\n"
                f"8. ì˜µì…˜(ìš©ëŸ‰, ìƒ‰ìƒ, í¬ê¸°, ìˆ˜ëŸ‰ ë“±) í¬í•¨ ê¸ˆì§€ (ì˜ˆ: í…€ë¸”ëŸ¬ 500ml)\n"
                f"9. Core keywordëŠ” ìµœëŒ€ 2íšŒë§Œ ì‚¬ìš©\n"
                f"ìµœì¢… ê²°ê³¼ëŠ” '{prefix}'ë¡œ ì‹œì‘í•˜ëŠ” 25ì ì´ìƒ 35ì ì´ë‚´ì˜ ìƒí’ˆëª… í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. "
                f"ì¹´í…Œë¶„ë¥˜í˜•ì‹: {category_format}\nCore keyword: {core_keyword}\në©”ì¸í‚¤ì›Œë“œ: {keyword}"
            )

            try:
                response = self.model.generate_content(prompt)
                product_name = response.text.strip()
                
                # prefixê°€ ë‘ ë²ˆ ë°˜ë³µë˜ë©´ í•œ ë²ˆë§Œ ë‚¨ê¸°ê¸°
                if product_name.count(prefix) > 1:
                    first = product_name.find(prefix)
                    product_name = prefix + product_name[first+len(prefix):]
                if not product_name.startswith(prefix):
                    product_name = f"{prefix} {product_name}"
                
                product_name = self._trim_product_name(product_name, min_len=25, max_len=35)
                product_name = self._clean_product_name(product_name)
                
                if len(product_name) < 25:
                    logger.warning(f"ìƒì„±ëœ ìƒí’ˆëª…ì´ 25ì ë¯¸ë§Œì…ë‹ˆë‹¤. ({product_name})")
                    
            except Exception as api_error:
                logger.error(f"ìƒí’ˆëª… ìƒì„± API ì˜¤ë¥˜: {str(api_error)}")
                product_name = f"{prefix}{keyword} ê³ ê¸‰ í’ˆì§ˆ ìƒí’ˆ"
            
            return product_name
            
        except Exception as e:
            logger.error(f"ìƒí’ˆëª… ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return self._generate_basic_product_name(keyword, category_format, core_keyword)

    def _generate_basic_product_name(self, keyword: str, category_format: str, core_keyword: str) -> str:
        """ê¸°ë³¸ ëª¨ë“œ: í‚¤ì›Œë“œ ê¸°ë°˜ ìƒí’ˆëª… ìƒì„±"""
        prefix_map = {
            'í…€ë¸”ëŸ¬': 'íœ´ëŒ€ìš©',
            'ì»¤í”¼': 'ì£¼ë°©',
            'ë³´ì˜¨ë³‘': 'ë³´ì˜¨',
            'ì‹ê¸°': 'ì£¼ë°©',
            'ê·¸ë¦‡': 'ì£¼ë°©',
            'ìº í•‘': 'ìº í•‘',
            'ì²­ì†Œ': 'ì‹¤ìš©ì ì¸',
            'ì •ë¦¬': 'í¸ë¦¬í•œ',
            'ë³´ê´€': 'ê¹”ë”í•œ'
        }
        
        prefix = 'ì‹¤ìš©ì ì¸'
        for key, value in prefix_map.items():
            if key in keyword or key in core_keyword:
                prefix = value
                break
        
        # ê¸°ë³¸ ìƒí’ˆëª… ìƒì„± ë¡œì§ ê°œì„ 
        base_name = f"{prefix}{keyword}"
        if len(base_name) < 20:
            base_name = f"{base_name} ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ì£¼ë°©ìš©í’ˆ"
        elif len(base_name) > 40:
            base_name = base_name[:40]
        
        return base_name

    def _select_best_prefix_word(self, category_format, core_keyword, keyword):
        """ì¹´í…Œë¶„ë¥˜í˜•ì‹, core keyword, ë©”ì¸í‚¤ì›Œë“œë¥¼ ì°¸ì¡°í•˜ì—¬ ìƒí’ˆëª… ì•ì— ë¶™ì¼ ê°€ì¥ ì í•©í•œ ìš©ë„/íŠ¹ì„±/ì¢…ë¥˜ ë‹¨ì–´ë¥¼ ì„ íƒ"""
        usage_words = [
            "íœ´ëŒ€ìš©", "ìº í•‘", "ì„ ë¬¼ìš©", "ë¯¸ë‹ˆ", "ëŒ€ìš©ëŸ‰", "ì°¨ëŸ‰ìš©", "ì•„ì´ìŠ¤", "ì†ì¡ì´", "ìŠ¤í…Œì¸ë¦¬ìŠ¤", "ì´ì¤‘", "ë³´ì˜¨ë³´ëƒ‰", "í™ˆ", "ì£¼ë°©", "ì‹ê¸°", "ê±°ìš¸", "ì„¸íŠ¸"
        ]
        context = f"{category_format} {core_keyword} {keyword}".lower()
        for word in usage_words:
            if word in context:
                if word == "ìº í•‘" and ("í™ˆì„¸íŠ¸" in context or "ì‹ê¸°" in context):
                    continue
                if word == "í™ˆ" and "ìº í•‘" in context:
                    continue
                return word
        if "ì‹ê¸°" in core_keyword or "ê·¸ë¦‡" in core_keyword:
            return "ì£¼ë°©"
        if "í™ˆì„¸íŠ¸" in core_keyword:
            return "í™ˆ"
        if "í…€ë¸”ëŸ¬" in core_keyword:
            return "íœ´ëŒ€ìš©"
        return random.choice(usage_words)

    def _trim_product_name(self, product_name, min_len=25, max_len=35):
        """ìƒí’ˆëª… ê¸¸ì´ ì¡°ì •"""
        words = product_name.split()
        result = ""
        for word in words:
            if len(result) + len(word) + (1 if result else 0) > max_len:
                break
            if result:
                result += " "
            result += word
        if len(result) < min_len and len(words) > len(result.split()):
            for word in words[len(result.split()):]:
                if len(result) + len(word) + 1 > max_len:
                    break
                result += " " + word
        return result

    def _clean_product_name(self, product_name):
        """ìƒí’ˆëª… ì •ë¦¬ - íŠ¹ìˆ˜ë¬¸ì, ì˜ì–´, ê¸°í˜¸ ì œê±°"""
        # íŠ¹ìˆ˜ë¬¸ì, ì˜ì–´, ê¸°í˜¸ ì œê±°
        cleaned = re.sub(r'[^ê°€-í£0-9 ]', '', product_name)
        # ì˜µì…˜ ê´€ë ¨ íŒ¨í„´ ì œê±°
        option_patterns = [
            r'\d+\s*(ml|l|ë¦¬í„°|cc|cm|mm|ì¸ì¹˜|inch)',
            r'\d+\s*(ê°œ|ì„¸íŠ¸|íŒ©|ì¥|ë²Œ|ìŒ|ì¼¤ë ˆ|ì¡°)',
            r'(ë¹¨ê°•|íŒŒë‘|ë¸”ë™|í™”ì´íŠ¸|ê·¸ë ˆì´|ì˜ë¡œìš°|í•‘í¬|ë¯¼íŠ¸|ë„¤ì´ë¹„|ì‹¤ë²„|ê³¨ë“œ|ë¸Œë¼ìš´|ì˜¤ë Œì§€|í¼í”Œ|ì²­ë¡|ì—°ë‘|ë‚¨ìƒ‰|íšŒìƒ‰|ë…¸ë‘|ì£¼í™©|ì´ˆë¡|ë³´ë¼|ë¶„í™|ì²­ìƒ‰|í°ìƒ‰|ê²€ì •|ê°ˆìƒ‰|ì€ìƒ‰|ê¸ˆìƒ‰|ì²­ë¡ìƒ‰|ì—°ë‘ìƒ‰|ë‚¨ìƒ‰|íšŒìƒ‰|ë…¸ë€ìƒ‰|ì£¼í™©ìƒ‰|ì´ˆë¡ìƒ‰|ë³´ë¼ìƒ‰|ë¶„í™ìƒ‰|ì²­ìƒ‰|í°ìƒ‰|ê²€ì •ìƒ‰|ê°ˆìƒ‰|ì€ìƒ‰|ê¸ˆìƒ‰)'
        ]
        for pattern in option_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        # ì—°ì†ëœ ê³µë°±ì„ í•œ ì¹¸ìœ¼ë¡œ
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned

    def _get_related_keywords(self, keyword, core_keyword, product_name):
        """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê´€ ê²€ìƒ‰ì–´ ì¶”ì¶œ - ìƒí’ˆëª… ì •ë³´ í™œìš©"""
        if not self.model:
            # ê¸°ë³¸ ì—°ê´€ê²€ìƒ‰ì–´ ë°˜í™˜ (ì¤‘ë³µ ì œê±° ì ìš©)
            base_keywords = [
                f"{core_keyword} ìš©í’ˆ", f"{core_keyword} ì œí’ˆ", f"{core_keyword} ì„¸íŠ¸",
                f"{core_keyword} ì •ë¦¬", f"{core_keyword} ë³´ê´€", f"{core_keyword} ì²­ì†Œ",
                f"{core_keyword} ê´€ë¦¬", f"{core_keyword} ë„êµ¬", f"{core_keyword} ì¥ë¹„",
                f"{core_keyword} ì •ë¦¬í•¨", f"{core_keyword} ê°€ë°©", f"{core_keyword} ë°•ìŠ¤",
                f"{core_keyword} ì •ë¦¬ëŒ€", f"{core_keyword} ë³´ê´€í•¨", f"{core_keyword} ì •ë¦¬ìš©í’ˆ",
                f"{core_keyword} ê´€ë¦¬ìš©í’ˆ", f"{core_keyword} ë„êµ¬í•¨", f"{core_keyword} ì¥ë¹„í•¨",
                f"{core_keyword} ì„¸íŠ¸í•¨", f"{core_keyword} ê³ ê¸‰ìš©í’ˆ"
            ]
            return self._remove_duplicates(base_keywords)
        
        try:
            # Geminiì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„± - ìƒí’ˆëª… ì •ë³´ í™œìš©
            prompt = f"""
            ë‹¤ìŒ ìƒí’ˆì— ëŒ€í•œ ë„¤ì´ë²„ ì‡¼í•‘ íƒœê·¸ 20ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            ë©”ì¸í‚¤ì›Œë“œ: {keyword}
            Core keyword: {core_keyword}
            ìƒì„±ëœ ìƒí’ˆëª…: {product_name}

            ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”:
            1. ë¸Œëœë“œ í‘œí˜„ ë‹¨ì–´ ì‚¬ìš©ê¸ˆì§€
            2. ì˜ì–´ë°œìŒ í‘œí˜„í•œê¸€ ì‚¬ìš©ê¸ˆì§€
            3. ì¤‘ë³µë‹¨ì–´ ì‚¬ìš©ê¸ˆì§€ (ë™ì¼í•œ ë‹¨ì–´ë‚˜ ì˜ë¯¸ê°€ ê°™ì€ ë‹¨ì–´ ë°˜ë³µ ê¸ˆì§€)
            4. ìƒí’ˆëª…ì— í¬í•¨ëœ ë‹¨ì–´ëŠ” ì¬ì‚¬ìš©ê¸ˆì§€ (ìƒí’ˆëª…: {product_name})
            5. ëª©ì ,ê¸°ëŠ¥,ëŒ€ìƒ,í¸ì˜ì„±,ì‚¬ì´ì¦ˆ,ë””ìì¸ ìš”ì†Œ,ì†Œì¬ ë° êµ¬ì¡° ê°•ì¡°
            6. íŠ¹ì •ì‚¬ìš©ìê·¸ë£¹,ì‚¬ìš©ìí™˜ê²½,ì¢…ë¥˜ ì¤‘ìš”ì„± í¬í•¨
            7. ê²€ìƒ‰ëŸ‰ì´ ë†’ì€ ìˆœì„œë¡œ ë°°ì¹˜
            8. ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ 20ê°œë§Œ ë°˜í™˜

            í˜•ì‹: íƒœê·¸1,íƒœê·¸2,íƒœê·¸3,...
            """

            # Gemini API í˜¸ì¶œ
            response = self.model.generate_content(prompt)
            
            # ì‘ë‹µì—ì„œ íƒœê·¸ ì¶”ì¶œ ë° ì¤‘ë³µ ì œê±°
            tags = response.text.strip().split(',')
            cleaned_tags = self._remove_duplicates(tags)
            
            # 20ê°œë¡œ ì œí•œ
            return cleaned_tags[:20]

        except Exception as e:
            logger.error(f"ì—°ê´€ê²€ìƒ‰ì–´ ìƒì„± API ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ íƒœê·¸ ë°˜í™˜ (ì¤‘ë³µ ì œê±° ì ìš©)
            fallback_tags = [
                f"{core_keyword} í…€ë¸”ëŸ¬", "ë³´ì˜¨ ë³´ëƒ‰ í…€ë¸”ëŸ¬", "ëŒ€ìš©ëŸ‰ í…€ë¸”ëŸ¬",
                "ì†ì¡ì´ í…€ë¸”ëŸ¬", "ì»¤í”¼ í…€ë¸”ëŸ¬", "ì•„ì´ìŠ¤ í…€ë¸”ëŸ¬", "ìº í•‘ í…€ë¸”ëŸ¬",
                "íœ´ëŒ€ìš© í…€ë¸”ëŸ¬", "ì„ ë¬¼ìš© í…€ë¸”ëŸ¬", "ë³´ì˜¨ë³‘", "íšŒì‚¬ í…€ë¸”ëŸ¬",
                "ì°¨ëŸ‰ìš© í…€ë¸”ëŸ¬", "ë¹¨ëŒ€ í…€ë¸”ëŸ¬", "ë¯¸ë‹ˆ í…€ë¸”ëŸ¬", "êµ­ì‚° í…€ë¸”ëŸ¬",
                "ì˜ˆìœ í…€ë¸”ëŸ¬", "ìŠ¤í… í…€ë¸”ëŸ¬", "ì´ì¤‘ í…€ë¸”ëŸ¬", "ë³´ëƒ‰ í…€ë¸”ëŸ¬",
                "ë³´ì˜¨ í…€ë¸”ëŸ¬"
            ]
            return self._remove_duplicates(fallback_tags)

    def _remove_duplicates(self, keywords):
        """ì—°ê´€ê²€ìƒ‰ì–´ì—ì„œ ì¤‘ë³µ ì œê±°"""
        if not keywords:
            return []
        
        # ê³µë°± ì œê±° ë° ì •ê·œí™”
        cleaned_keywords = []
        for keyword in keywords:
            cleaned = keyword.strip()
            if cleaned and len(cleaned) > 0:
                cleaned_keywords.append(cleaned)
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        seen = set()
        unique_keywords = []
        for keyword in cleaned_keywords:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì¤‘ë³µ ì²´í¬
            keyword_lower = keyword.lower()
            if keyword_lower not in seen:
                seen.add(keyword_lower)
                unique_keywords.append(keyword)
        
        return unique_keywords

class CategoryMapper:
    """ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í´ë˜ìŠ¤ - ë²¡í„°í™” ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­"""
    
    def __init__(self):
        self.category_map = {}
        self.vectorized_data = None
        self.cache_file = os.path.join(SCRIPT_DIR, 'data', 'category_vector_cache.pkl')
        self.cache_expiry_days = 7  # ìºì‹œ ìœ íš¨ ê¸°ê°„ (ì¼)
        
    def load_category_data(self):
        """naver.xlsx íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™” (ìºì‹œ í™œìš©)"""
        try:
            # ìºì‹œëœ ë²¡í„°í™” ë°ì´í„° í™•ì¸
            if self._load_cached_data():
                logger.info("ìºì‹œëœ ë²¡í„°í™” ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return True
            
            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
            naver_file = os.path.join(SCRIPT_DIR, "data", "naver.xlsx")
            logger.info(f"ì¹´í…Œê³ ë¦¬ íŒŒì¼ ê²½ë¡œ: {naver_file}")
            
            if not os.path.exists(naver_file):
                logger.warning(f"naver.xlsx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {naver_file}")
                return False
            
            df = pd.read_excel(naver_file)
            
            # 'ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹' ì—´ì´ ì—†ìœ¼ë©´ ìƒì„±
            if 'ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹' not in df.columns:
                df['ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹'] = df.apply(
                    lambda row: '>'.join(
                        [str(row['1ì°¨ë¶„ë¥˜']), str(row['2ì°¨ë¶„ë¥˜']), str(row['3ì°¨ë¶„ë¥˜']), str(row['4ì°¨ë¶„ë¥˜'])]
                        if not pd.isnull(row['4ì°¨ë¶„ë¥˜']) else
                        [str(row['1ì°¨ë¶„ë¥˜']), str(row['2ì°¨ë¶„ë¥˜']), str(row['3ì°¨ë¶„ë¥˜'])]
                    ),
                    axis=1
                )
            
            if 'ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹' not in df.columns or 'catecode' not in df.columns:
                logger.warning("naver.xlsx íŒŒì¼ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ì¹´í…Œê³ ë¦¬ ë§µ ìƒì„±
            self.category_map = dict(zip(df['ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹'], df['catecode']))
            
            # ë²¡í„°í™” ìˆ˜í–‰
            self.vectorized_data = self._vectorize_categories(df)
            
            # ë²¡í„°í™”ëœ ë°ì´í„° ìºì‹œ ì €ì¥
            self._save_cached_data()
            
            logger.info(f"ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.category_map)}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return False

    def _vectorize_categories(self, df):
        """ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë²¡í„°í™”"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            
            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))
            vectors = vectorizer.fit_transform(df['ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹'])
            
            return {
                'vectors': vectors,
                'vectorizer': vectorizer,
                'categories': df['ì¹´í…Œê³ ë¦¬ë¶„ë¥˜í˜•ì‹'].tolist(),
                'codes': df['catecode'].tolist(),
                'last_updated': datetime.now()
            }
        except ImportError:
            logger.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë§¤ì¹­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None

    def _save_cached_data(self):
        """ë²¡í„°í™”ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            import pickle
            cache_data = {
                'category_map': self.category_map,
                'vectorized_data': self.vectorized_data,
                'timestamp': datetime.now()
            }
            
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            logger.info("ë²¡í„°í™”ëœ ë°ì´í„°ê°€ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    def _load_cached_data(self):
        """ìºì‹œëœ ë²¡í„°í™” ë°ì´í„° ë¡œë“œ"""
        try:
            import pickle
            if not os.path.exists(self.cache_file):
                return False
            
            # ìºì‹œ íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            cache_time = datetime.fromtimestamp(os.path.getmtime(self.cache_file))
            if datetime.now() - cache_time > timedelta(days=self.cache_expiry_days):
                logger.info("ìºì‹œê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
                return False
            
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            self.category_map = cache_data['category_map']
            self.vectorized_data = cache_data['vectorized_data']
            
            return True
            
        except Exception as e:
            logger.error(f"ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def find_category_code(self, category_format: str) -> tuple:
        """ì¹´í…Œê³ ë¦¬ í˜•ì‹ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ ì°¾ê¸°"""
        if not category_format:
            return '50000000', False
        
        try:
            # 1. ì •í™•í•œ ë§¤ì¹­
            if category_format in self.category_map:
                return self.category_map[category_format], False
            
            # 2. ë²¡í„°í™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ë§¤ì¹­
            if self.vectorized_data:
                from sklearn.metrics.pairwise import cosine_similarity
                input_vector = self.vectorized_data['vectorizer'].transform([category_format])
                similarities = cosine_similarity(input_vector, self.vectorized_data['vectors'])
                
                most_similar_idx = similarities.argmax()
                similarity_score = similarities[0][most_similar_idx]
                
                if similarity_score > 0.95:
                    matched_code = self.vectorized_data['codes'][most_similar_idx]
                    return matched_code, False
            
            # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ë³¸ê°’
            default_codes = {
                'í…€ë¸”ëŸ¬': '50000000',
                'ì»¤í”¼': '50000000',
                'ë³´ì˜¨ë³‘': '50000000',
                'ì‹ê¸°': '50000000',
                'ê·¸ë¦‡': '50000000',
                'ìº í•‘': '50000000',
                'ì£¼ë°©': '50000000',
                'ë³´ì˜¨': '50000000',
                'ë³´ëƒ‰': '50000000'
            }
            
            for keyword, default_code in default_codes.items():
                if keyword in category_format:
                    return default_code, True
            
            # 4. ì™„ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ ì£¼ë°©ìš©í’ˆ ì½”ë“œ
            return '50000000', True
            
        except Exception as e:
            return '50000000', True

def check_api_keys():
    """API í‚¤ ì„¤ì • í™•ì¸"""
    logger.info("=== API í‚¤ í™•ì¸ ì¤‘ ===")
    logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {SCRIPT_DIR}")
    logger.info(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    # .env íŒŒì¼ ì¡´ì¬ í™•ì¸
    env_file = os.path.join(SCRIPT_DIR, ".env")
    if os.path.exists(env_file):
        logger.info(f".env íŒŒì¼ ì¡´ì¬: {env_file}")
    else:
        logger.info(f".env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {env_file}")
        logger.info("   â†’ ì½”ë“œ ë‚´ ì§ì ‘ ì„¤ì • ë°©ì‹ ì‚¬ìš©")
    
    api_keys_status = {
        'gemini': False,
        'naver': False
    }
    
    # Gemini API í‚¤ í™•ì¸
    if not GEMINI_API_KEY or GEMINI_API_KEY == 'your_gemini_api_key_here':
        logger.warning("âŒ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("   í•´ê²° ë°©ë²•:")
        logger.info("   1. í™˜ê²½ë³€ìˆ˜: .env íŒŒì¼ì— GEMINI_API_KEY ì„¤ì •")
        logger.info("   2. ì§ì ‘ì„¤ì •: processor.pyì˜ DIRECT_GEMINI_API_KEY ë³€ìˆ˜ì— ì…ë ¥")
        logger.info("   https://makersuite.google.com/app/apikey ì—ì„œ ë°œê¸‰")
        logger.warning("   â†’ ê¸°ë³¸ ìƒí’ˆëª… ìƒì„± ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    else:
        logger.info(f"âœ… Gemini API í‚¤ í™•ì¸ë¨: {GEMINI_API_KEY[:10]}...")
        api_keys_status['gemini'] = True
    
    # ë„¤ì´ë²„ API í‚¤ í™•ì¸
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET or NAVER_CLIENT_ID == 'your_naver_client_id_here':
        logger.warning("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("   í•´ê²° ë°©ë²•:")
        logger.info("   1. í™˜ê²½ë³€ìˆ˜: .env íŒŒì¼ì— NAVER_CLIENT_ID, NAVER_CLIENT_SECRET ì„¤ì •")
        logger.info("   2. ì§ì ‘ì„¤ì •: processor.pyì˜ DIRECT_NAVER_CLIENT_ID, DIRECT_NAVER_CLIENT_SECRET ë³€ìˆ˜ì— ì…ë ¥")
        logger.info("   https://developers.naver.com/apps/#/list ì—ì„œ ë°œê¸‰")
        logger.warning("   â†’ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    else:
        logger.info(f"âœ… ë„¤ì´ë²„ API í‚¤ í™•ì¸ë¨: Client ID: {NAVER_CLIENT_ID[:10]}...")
        api_keys_status['naver'] = True
    
    if api_keys_status['gemini'] and api_keys_status['naver']:
        logger.info("âœ… ëª¨ë“  API í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("   â†’ AI ê¸°ë°˜ ê³ í’ˆì§ˆ ìƒí’ˆëª… ìƒì„± ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    elif api_keys_status['gemini']:
        logger.info("âœ… Gemini API í‚¤ë§Œ ì„¤ì •ë¨")
        logger.info("   â†’ AI ê¸°ë°˜ ìƒí’ˆëª… ìƒì„± + ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    else:
        logger.warning("âš ï¸  API í‚¤ê°€ ì—†ì–´ ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        logger.info("   â†’ ë¹ ë¥¸ ì²˜ë¦¬ (API í˜¸ì¶œ ì—†ìŒ) + ê¸°ë³¸ í…œí”Œë¦¿ ìƒí’ˆëª…")
    
    return api_keys_status['gemini'] and api_keys_status['naver']

# ì „ì—­ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
qname_processor = QNameProcessor()

def process_file(file_path: str) -> dict:
    """íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ - ê°„ì†Œí™”ëœ ì¸í„°í˜ì´ìŠ¤"""
    return qname_processor.process_excel_file(file_path)

if __name__ == "__main__":
    logger.info("ìƒí’ˆëª… ë° í‚¤ì›Œë“œ ìƒì„± í”„ë¡œì„¸ì„œ ì‹œì‘")
    logger.info("=" * 50)
    
    # API í‚¤ í™•ì¸
    check_api_keys()
    
    # ì…ë ¥ íŒŒì¼ ì°¾ê¸°
    excel_files = [f for f in os.listdir('.') if f.endswith('.xlsx') and not f.startswith('ê°€ê³µì™„ë£Œ')]
    
    if not excel_files:
        logger.error("ì²˜ë¦¬í•  ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("   í˜„ì¬ í´ë”ì— ì—‘ì…€ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        exit(1)
    
    input_file = excel_files[0]
    logger.info(f"ì²˜ë¦¬í•  íŒŒì¼: {input_file}")
    
    # íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰
    result = process_file(input_file)
    
    if result['success']:
        logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {result['output_file']}")
        logger.info(f"ì„±ê³µ: {result['success_count']}í–‰, ì‹¤íŒ¨: {result['error_count']}í–‰")
    else:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
        exit(1) 